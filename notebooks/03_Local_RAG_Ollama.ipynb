{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096624cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n",
      "‚úì Ready for local offline RAG!\n",
      "\n",
      "Python version: 3.13.0 (main, Oct 16 2024, 08:05:40) [Clang 18.1.8 ]\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain Document Loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Integration\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# ChromaDB Vector Store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Ready for local offline RAG!\")\n",
    "print(f\"\\nPython version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95df7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED      \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    7 months ago     \n",
      "deepseek-r1:latest         0a8c26691023    4.7 GB    9 months ago     \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    14 months ago    \n"
     ]
    }
   ],
   "source": [
    "# Check Ollama is running and list available models\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d502de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ollama connection...\n",
      "\n",
      "‚úì Ollama is working!\n",
      "Response: <think>\n",
      "Okay, so the user wants me to say \"Hello! I am running locally on your machine!\" in response to their greeting. Let me make sure I understand what they're asking for.\n",
      "\n",
      "First, they said, \"Say 'Hello! I am running locally on your machine!'\". That seems straightforward‚Äîthey want a specific response. But wait, are they testing my capabilities or trying to see if I can follow instructions accurately?\n",
      "\n",
      "I should consider the context here. If someone is greeting me and wants me to respond with that exact phrase, maybe they're checking how I handle user interactions. It could also be part of a larger system setup where they need confirmation from an AI.\n",
      "\n",
      "Now, thinking about my response options. The initial reply was \"Hello! How can I assist you today?\" which seems appropriate for general interaction. But the user specifically wants to say another phrase. Maybe they want me to repeat that exact message back to them or use it in a conversation.\n",
      "\n",
      "I should clarify if they need me to output that specific text as part of my response. If so, then perhaps I can structure my reply to include both the greeting and their statement. Alternatively, maybe they just want confirmation that I've received their message correctly.\n",
      "\n",
      "Also, considering the technical aspect, running locally on a machine might refer to programming environments like Python with libraries such as TensorFlow or PyTorch. So, if this is part of an application setup, ensuring that my response acknowledges both parts could be helpful.\n",
      "\n",
      "In any case, it's important to respond clearly and accurately based on their instructions without overcomplicating things. I'll make sure to include the exact phrase they mentioned in my reply.\n",
      "</think>\n",
      "\n",
      "Hello! I am running locally on your machine! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama connection with a simple query\n",
    "print(\"Testing Ollama connection...\\n\")\n",
    "\n",
    "try:\n",
    "    test_llm = ChatOllama(model=\"deepseek-r1\", temperature=0)\n",
    "    response = test_llm.invoke(\"Say 'Hello! I am running locally on your machine!'\")\n",
    "    \n",
    "    print(\"‚úì Ollama is working!\")\n",
    "    print(f\"Response: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nMake sure Ollama is running. Try: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008c3b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 15 pages from '../resources/pdfs/attention.pdf'\n",
      "\n",
      "--- First Page Preview ---\n",
      "Content (first 300 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"../resources/pdfs/attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ö†Ô∏è  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information\n",
    "    print(f\"‚úì Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Page Preview ---\")\n",
    "    print(f\"Content (first 300 chars): {documents[0].page_content[:300]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43db6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 15 documents into 49 chunks\n",
      "\n",
      "Average chunk size: 873 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 986 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "\n",
      "Chunk 2 (length: 944 chars):\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more pa...\n",
      "\n",
      "Chunk 3 (length: 986 chars):\n",
      "‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Characters per chunk\n",
    "    chunk_overlap=128,      # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split on paragraphs, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display results\n",
    "avg_chunk_size = sum(len(chunk.page_content) for chunk in chunks) / len(chunks)\n",
    "\n",
    "print(f\"‚úì Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {avg_chunk_size:.0f} characters\")\n",
    "\n",
    "# Preview chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16c4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nomic-embed-text embeddings...\n",
      "\n",
      "‚úì Embeddings model: nomic-embed-text\n",
      "‚úì Embedding dimension: 768\n",
      "‚úì Sample embedding (first 10 values): [0.032470614, 0.060767617, -0.1661029, -0.08210531, 0.04330724, -0.02606512, 0.051619876, -0.015222877, -0.0082509415, -0.028311769]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 768-dimensional vector\n",
      "‚ÑπÔ∏è  All processing happens locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama Embeddings with nomic-embed-text\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    # base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "print(\"Testing nomic-embed-text embeddings...\\n\")\n",
    "sample_text = \"This is a test sentence for embeddings.\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model: nomic-embed-text\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector\")\n",
    "print(f\"‚ÑπÔ∏è  All processing happens locally on your machine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c462607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ChromaDB vector store from 49 chunks...\n",
      "This may take a minute...\n",
      "\n",
      "‚úì ChromaDB vector store created successfully!\n",
      "‚úì Indexed 49 document chunks\n",
      "‚úì Stored at: ../vectorstores/chroma_db\n",
      "\n",
      "‚ÑπÔ∏è  Vector store persisted to disk - you can reload it later!\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB vector store\n",
    "print(f\"Creating ChromaDB vector store from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Set persistent directory\n",
    "persist_directory = \"../vectorstores/chroma_db\"\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"local_rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì ChromaDB vector store created successfully!\")\n",
    "print(f\"‚úì Indexed {len(chunks)} document chunks\")\n",
    "print(f\"‚úì Stored at: {persist_directory}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Vector store persisted to disk - you can reload it later!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5f743d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base...\n",
      "  Source: Page 8\n",
      "\n",
      "Document 2:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Source: Page 13\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d...\n",
      "  Source: Page 5\n",
      "\n",
      "Document 4:\n",
      "  Content preview: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "re...\n",
      "  Source: Page 12\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Source: Page {doc.metadata.get('page', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace3e8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM configured successfully\n",
      "  - Model: deepseek-r1:latest  (local)\n",
      "  - Temperature: 0 (deterministic)\n",
      "\n",
      "LLM Test Response: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! I am DeepSeek running locally. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1\",\n",
    "    temperature=0,          # Deterministic responses (0 = focused, 1 = creative)\n",
    "    # num_predict=2000,     # Max tokens to generate\n",
    "    # top_k=40,             # Top-k sampling\n",
    "    # top_p=0.9,            # Top-p (nucleus) sampling\n",
    ")\n",
    "\n",
    "print(\"‚úì LLM configured successfully\")\n",
    "print(f\"  - Model: deepseek-r1:latest  (local)\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "\n",
    "# Test LLM\n",
    "test_response = llm.invoke(\"Say 'Hello! I am DeepSeek running locally!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a143d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created successfully using LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks (local ChromaDB)\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question formatted with prompt template\n",
      "  5. Local LLM (gemma3:1b) generates answer\n",
      "  6. Answer parsed and returned\n",
      "\n",
      "üîí Everything runs locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "# Define prompt template\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format docs\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with local LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created successfully using LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks (local ChromaDB)\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question formatted with prompt template\")\n",
    "print(\"  5. Local LLM (gemma3:1b) generates answer\")\n",
    "print(\"  6. Answer parsed and returned\")\n",
    "print(\"\\nüîí Everything runs locally on your machine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f08c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or contribution of this document?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "<think>\n",
      "Okay, so I need to figure out the main topic or contribution of this document based on the context provided. Let me start by reading through the context carefully.\n",
      "\n",
      "The context seems to be discussing some technical aspects related to machine translation models, specifically focusing on attention mechanisms and learning rates. There are mentions of layers in a neural network model, like Layer 5 and Layer 6, which suggests that this is part of a larger architecture, possibly an encoder-decoder model used for translating English to German.\n",
      "\n",
      "The sections mentioned include section 5.4 and section 3.2, which probably detail different parts of the model's architecture. The context talks about attention layers in the encoder and decoder, mentions learning rates and beam size as parameters that were adjusted during training but remained unchanged otherwise. There are also figures showing attention visualizations, which help explain how the model processes certain phrases or sentences.\n",
      "\n",
      "The document seems to be a technical report or paper detailing experiments and results related to improving machine translation models. The focus is on enhancing the model's ability to handle long-distance dependencies in text through better attention mechanisms and possibly optimizing training parameters like learning rates and beam size for inference.\n",
      "\n",
      "Putting this together, the main topic would likely be about advancing machine translation techniques by improving attention mechanisms and optimizing certain hyperparameters to make the model more effective. The contribution is probably a method or set of changes that enhances the model's performance in translating text, especially in handling complex sentence structures.\n",
      "</think>\n",
      "\n",
      "The document focuses on enhancing machine translation models by improving attention mechanisms and optimizing training parameters such as learning rates and beam size. Its main contribution is advancing the architecture to better handle long-distance dependencies in text, leading to improved translation accuracy.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Page: 8\n",
      "  Content: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Page: 12\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Page: 13\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Page: 2\n",
      "  Content: Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "att...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question\n",
    "query1 = \"What is the main topic or contribution of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Show source documents\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b724b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key technical contributions or innovations mentioned?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "<think>\n",
      "Okay, so I need to summarize the key technical contributions or innovations from the given context about the Transformer architecture. Let me read through the context carefully and pick out the important points.\n",
      "\n",
      "First, the context mentions that Jakob proposed replacing RNNs with self-attention and started evaluating this idea. That's a big contribution because it introduced the Transformer model, which uses attention mechanisms instead of recurrence. Then, Ashish, along with Illia, designed and implemented the first Transformer models. They were crucial in every aspect, so their work was foundational.\n",
      "\n",
      "Noam proposed several key components: scaled dot-product attention, multi-head attention, and a parameter-free position representation. These are core innovations because they improved how the model handles attention and positional information without adding parameters, which is efficient.\n",
      "\n",
      "Niki designed, implemented, tuned, and evaluated numerous model variants in tensor2tensor. That shows significant work in experimenting with different configurations to find what works best. Llion also experimented with new models, was responsible for earlier codebases, improved efficiency, and did visualizations‚Äîso they contributed to both the development and evaluation processes.\n",
      "\n",
      "Lukasz and Aidan replaced the previous codebase with tensor2tensor, which likely improved results and accelerated research by providing a better framework. Their work was important in advancing the field.\n",
      "\n",
      "Looking at Table 3, it shows variations of the Transformer architecture with different parameters like d (dimension), model depth, etc., each affecting performance metrics like perplexity and BLEU scores. These variations demonstrate how tweaking architectural choices can impact model performance, which is a key point about their innovations.\n",
      "\n",
      "The context also mentions that they estimated floating-point operations by considering training time, GPUs used, and GPU capacity, providing insights into computational efficiency. This method adds another layer of understanding the technical contributions in terms of resource usage and optimization.\n",
      "\n",
      "So putting it all together, the main points are the introduction of self-attention by Jakob, key components from Noam, significant experimentation by Niki, Llion, and Lukasz/Aidan, and the empirical evaluation shown in Table 3 along with computational analysis.\n",
      "</think>\n",
      "\n",
      "The key technical contributions of this work include:\n",
      "\n",
      "1. **Introduction of Self-Attention**: Jakob proposed replacing Recurrent Neural Networks (RNNs) with self-attention mechanisms, laying the foundation for the Transformer architecture.\n",
      "\n",
      "2. **Foundational Contributions by Ashish and Illia**: Their design and implementation of the first Transformer models were pivotal in advancing the field.\n",
      "\n",
      "3. **Core Innovations by Noam**: The introduction of scaled dot-product attention, multi-head attention, and a parameter-free position representation significantly enhanced the model's efficiency and effectiveness.\n",
      "\n",
      "4. **Extensive Experimentation**: Niki, Llion, and Lukasz/Aidan contributed through extensive experimentation with various model configurations using tensor2tensor, improving results and accelerating research.\n",
      "\n",
      "5. **Empirical Evaluation**: Table 3 demonstrates how different architectural parameters affect performance metrics like perplexity and BLEU scores, providing insights into optimal design choices.\n",
      "\n",
      "6. **Efficiency Analysis**: The method of estimating floating-point operations based on training time, GPU usage, and capacity highlights the importance of computational efficiency in model development.\n",
      "\n",
      "These contributions collectively advanced the Transformer architecture, setting a robust framework for subsequent research and applications.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key technical contributions or innovations mentioned?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0586d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about the methodology or approach?\n",
      "\n",
      "Processing locally...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "<think>\n",
      "Okay, so I need to figure out what specific details are mentioned about the methodology or approach in the given context. Let me read through the context carefully.\n",
      "\n",
      "The first part talks about a Law not being perfect but its application should be just‚Äîthis is an opinionated statement from someone. Then there's some mention of Figure 4 with attention heads in layer 5, which seems to relate to how the model processes information. It mentions two attention heads involved in anaphora resolution and shows full attentions for head 5, as well as isolated attentions from the word 'its' for heads 5 and 6.\n",
      "\n",
      "Further down, there are references to sections like section 3.2 about Multi-Head Attention, which is a method used in Transformers. The context also discusses self-attention mechanisms, their applications in tasks like reading comprehension, summarization, entailment, and sentence representations. It mentions that the Transformer uses entirely self-attention without recurrence.\n",
      "\n",
      "Additionally, there are citations to various papers on attention models, including works by Marcus et al., McClosky et al., Parikh et al., Paulus et al., Petrov et al., and others. These seem to be supporting the use of different attention mechanisms in their approach.\n",
      "\n",
      "So putting it all together, the methodology involves using self-attention as described, employing Multi-Head Attention for better performance, and referencing several studies that validate these approaches across various tasks.\n",
      "</think>\n",
      "\n",
      "The context outlines a methodology that primarily uses self-attention mechanisms within Transformers. Key details include:\n",
      "\n",
      "1. **Self-Attention**: The approach relies on self-attention to process sequences without recurrence, as seen in models like the Transformer.\n",
      "\n",
      "2. **Multi-Head Attention**: This technique is employed for enhanced performance, allowing multiple attention heads to capture diverse contextual information.\n",
      "\n",
      "3. **Applications and Validation**: The effectiveness of these mechanisms is supported by studies across tasks such as reading comprehension, summarization, entailment, and sentence representation, citing works by Marcus et al., Petrov et al., Parikh et al., among others.\n",
      "\n",
      "This methodology represents an advancement in sequence modeling without traditional recurrence, leveraging attention for improved performance.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3: Your custom question\n",
    "custom_query = \"What specific details are mentioned about the methodology or approach?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing locally...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36906043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Question: What are the main findings or results?\n",
      "================================================================================\n",
      "\n",
      "Answer: <think>\n",
      "Okay, so I need to figure out what the main findings or results are from the given context about their research on the Transformer architecture. Let me start by reading through the context carefully.\n",
      "\n",
      "First, there's mention of experiments conducted at Google Brain and Google Research during the NIPS 2017 conference. They looked into variations of the Transformer model, specifically focusing on different configurations like varying the number of layers (N), embedding dimensions (d), hidden dimensions (dff), attention heads (h), key dimension (dk), value dimension (dv), dropout rate (Pdrop), label smoothing (œµls), training perplexity (train PPL), and BLEU scores during inference.\n",
      "\n",
      "Looking at Table 3, there are several configurations labeled from A to E. Each configuration has different values for d, dff, h, dk, dv, Pdrop, œµls, and the resulting train PPL and BLEU scores. For example, Configuration A uses a smaller model with fewer layers (N=6) but lower embedding dimensions (512), which resulted in higher perplexity (4.92) and BLEU score of 25.8%. Configuration B has more layers (N=16) with even smaller dimensions, leading to slightly better results.\n",
      "\n",
      "Another key point is the decoder structure: it's composed of N=6 identical layers, each with an additional multi-head attention sub-layer that prevents positions from attending to subsequent ones through masking and offsetting the output embeddings. This helps in generating translations without looking ahead.\n",
      "\n",
      "The main findings seem to be about how different architectural choices affect model performance. Smaller models (like Configuration A) perform well on the training set but might not generalize as well, indicated by their lower perplexity compared to larger models which have higher perplexity. The BLEU scores also vary, with some configurations performing better than others.\n",
      "\n",
      "Additionally, the use of multi-head attention in the decoder and the masking technique are important findings because they improve the model's ability to handle translation tasks without relying on future tokens.\n",
      "</think>\n",
      "\n",
      "The main findings from the research on the Transformer architecture include:\n",
      "\n",
      "1. **Model Configurations**: Various architectural variations were tested with different dimensions (d, dff), layers (N=6), attention heads (h), and dropout rates (Pdrop). Configuration A, with smaller dimensions (512) and fewer layers (N=6), achieved a training perplexity of 4.92 and a BLEU score of 25.8%. Larger models showed higher perplexity but comparable or slightly better BLEU scores.\n",
      "\n",
      "2. **Decoder Structure**: The decoder uses N=6 identical layers with multi-head attention, employing masking to prevent future token dependency during translation, enhancing the model's ability to generate translations without relying on future tokens.\n",
      "\n",
      "3. **Performance Trade-offs**: Smaller models demonstrated good performance on the training set (newstest2013) but may not generalize as effectively, while larger models offered better generalization despite higher perplexity.\n",
      "\n",
      "These findings highlight the impact of architectural choices on model performance and suggest a balance between model size and generalization in Transformer-based translation tasks.\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so I need to figure out what the main findings or results are from the given context about their research on the Transformer architecture. Let me start by reading through the context carefully.\\n\\nFirst, there's mention of experiments conducted at Google Brain and Google Research during the NIPS 2017 conference. They looked into variations of the Transformer model, specifically focusing on different configurations like varying the number of layers (N), embedding dimensions (d), hidden dimensions (dff), attention heads (h), key dimension (dk), value dimension (dv), dropout rate (Pdrop), label smoothing (œµls), training perplexity (train PPL), and BLEU scores during inference.\\n\\nLooking at Table 3, there are several configurations labeled from A to E. Each configuration has different values for d, dff, h, dk, dv, Pdrop, œµls, and the resulting train PPL and BLEU scores. For example, Configuration A uses a smaller model with fewer layers (N=6) but lower embedding dimensions (512), which resulted in higher perplexity (4.92) and BLEU score of 25.8%. Configuration B has more layers (N=16) with even smaller dimensions, leading to slightly better results.\\n\\nAnother key point is the decoder structure: it's composed of N=6 identical layers, each with an additional multi-head attention sub-layer that prevents positions from attending to subsequent ones through masking and offsetting the output embeddings. This helps in generating translations without looking ahead.\\n\\nThe main findings seem to be about how different architectural choices affect model performance. Smaller models (like Configuration A) perform well on the training set but might not generalize as well, indicated by their lower perplexity compared to larger models which have higher perplexity. The BLEU scores also vary, with some configurations performing better than others.\\n\\nAdditionally, the use of multi-head attention in the decoder and the masking technique are important findings because they improve the model's ability to handle translation tasks without relying on future tokens.\\n</think>\\n\\nThe main findings from the research on the Transformer architecture include:\\n\\n1. **Model Configurations**: Various architectural variations were tested with different dimensions (d, dff), layers (N=6), attention heads (h), and dropout rates (Pdrop). Configuration A, with smaller dimensions (512) and fewer layers (N=6), achieved a training perplexity of 4.92 and a BLEU score of 25.8%. Larger models showed higher perplexity but comparable or slightly better BLEU scores.\\n\\n2. **Decoder Structure**: The decoder uses N=6 identical layers with multi-head attention, employing masking to prevent future token dependency during translation, enhancing the model's ability to generate translations without relying on future tokens.\\n\\n3. **Performance Trade-offs**: Smaller models demonstrated good performance on the training set (newstest2013) but may not generalize as effectively, while larger models offered better generalization despite higher perplexity.\\n\\nThese findings highlight the impact of architectural choices on model performance and suggest a balance between model size and generalization in Transformer-based translation tasks.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interactive Q&A\n",
    "def ask_question(question):\n",
    "    \"\"\"Ask a question to the RAG system.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Try it out!\n",
    "# Change the question below to ask anything about your document\n",
    "my_question = \"What are the main findings or results?\"\n",
    "ask_question(my_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d008cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2993545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n",
      "‚úì Compatible with LangChain 1.0+\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # For macOS users to avoid OpenMP error, specifically for FIASS\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variable management - for secure API key handling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Document Loaders - for loading PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# LangChain Text Splitters - for breaking documents into manageable chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# OpenAI Integration - for embeddings and LLM\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Gemini Integration - for embeddings and LLM\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Vector Store - FAISS for efficient similarity search\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"‚úì All imports successful!\")\n",
    "print(\"‚úì Compatible with LangChain 1.0+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c8f0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI API Key loaded successfully!\n",
      "‚úì Key starts with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found!\")\n",
    "    print(\"Please set it in .env file or uncomment the line below:\")\n",
    "else:\n",
    "    print(\"‚úì OpenAI API Key loaded successfully!\")\n",
    "    print(f\"‚úì Key starts with: {os.getenv('OPENAI_API_KEY')[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781fa6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 15 pages from '../resources/pdfs/attention.pdf'\n",
      "\n",
      "--- First Document Preview ---\n",
      "Content (first 500 chars): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó ‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz ...\n",
      "\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Total characters across all pages: 39,587\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION: Update this path to your PDF file =====\n",
    "pdf_path = \"../resources/pdfs/attention.pdf\"  # Change this to your PDF file path\n",
    "# =============================================================\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"‚ö†Ô∏è  ERROR: File '{pdf_path}' not found!\")\n",
    "    print(\"Please update the pdf_path variable with your PDF file location.\")\n",
    "else:\n",
    "    # Initialize the PDF loader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    \n",
    "    # Load all pages from the PDF\n",
    "    # Each page becomes a separate Document object\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Display information about loaded documents\n",
    "    print(f\"‚úì Loaded {len(documents)} pages from '{pdf_path}'\")\n",
    "    print(f\"\\n--- First Document Preview ---\")\n",
    "    print(f\"Content (first 500 chars): {documents[0].page_content[:500]}...\")\n",
    "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
    "    print(f\"\\nTotal characters across all pages: {sum(len(doc.page_content) for doc in documents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c0c717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files\n",
      "  ‚úì Loaded 15 pages from attention.pdf\n",
      "  ‚úì Loaded 19 pages from rag.pdf\n",
      "  ‚úì Loaded 21 pages from ragsurvey.pdf\n",
      "\n",
      "Total pages loaded: 55\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading multiple PDFs from a directory\n",
    "\n",
    "pdf_directory = \"../resources/pdfs\"  # Directory containing your PDFs\n",
    "all_documents = []\n",
    "\n",
    "if os.path.exists(pdf_directory):\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs = loader.load()\n",
    "        all_documents.extend(docs)\n",
    "        print(f\"  ‚úì Loaded {len(docs)} pages from {pdf_file.name}\")\n",
    "    \n",
    "    print(f\"\\nTotal pages loaded: {len(all_documents)}\")\n",
    "    documents = all_documents  # Use this for the rest of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0954aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Split 15 documents into 49 chunks\n",
      "\n",
      "Average chunk size: 873 characters\n",
      "\n",
      "--- Chunk Examples ---\n",
      "\n",
      "Chunk 1 (length: 986 chars):\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 2 (length: 944 chars):\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more pa...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Chunk 3 (length: 986 chars):\n",
      "‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Tra...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter with recommended settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Maximum characters per chunk (roughly 200-250 tokens)\n",
    "    chunk_overlap=128,      # Characters overlap between chunks (maintains context)\n",
    "    length_function=len,    # Function to measure chunk length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraphs first, then lines, etc.\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "# This creates smaller, manageable pieces while preserving semantic meaning\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display splitting results\n",
    "print(f\"‚úì Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nAverage chunk size: {sum(len(chunk.page_content) for chunk in chunks) / len(chunks):.0f} characters\")\n",
    "\n",
    "# Preview a few chunks\n",
    "print(f\"\\n--- Chunk Examples ---\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} (length: {len(chunk.page_content)} chars):\")\n",
    "    print(f\"{chunk.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588d7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings model initialized: text-embedding-3-small\n",
      "‚úì Embedding dimension: 1536\n",
      "‚úì Sample embedding (first 10 values): [0.020318197086453438, -0.003171295393258333, -0.0005874512135051191, 0.004569540731608868, -0.014964818023145199, -0.03400970622897148, 0.017634661868214607, 0.01959254778921604, 0.0012707430869340897, 0.0059489598497748375]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 1536-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI Embeddings\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # Latest, cost-effective embedding model\n",
    "    # Alternative: \"text-embedding-3-large\" for better quality\n",
    ")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate embeddings.\"\n",
    "sample_embedding = openai_embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"‚úì Embeddings model initialized: text-embedding-3-small\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1701a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings model initialized: gemini-embedding-001\n",
      "‚úì Embedding dimension: 768\n",
      "‚úì Sample embedding (first 10 values): [-0.03882233425974846, 0.01693294569849968, 0.011709309183061123, -0.08021727204322815, 0.010542317293584347, -0.0008019016240723431, 0.031178826466202736, 0.025036286562681198, 0.015625935047864914, -0.015515029430389404]\n",
      "\n",
      "‚ÑπÔ∏è  Each chunk will be converted to a 768-dimensional vector for similarity search\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini Embeddings\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"gemini-embedding-001\",  # Latest, cost-effective embedding model\n",
    ")\n",
    "\n",
    "# Test the embeddings with a sample text\n",
    "sample_text = \"This is a test sentence to demonstrate  gemnini embeddings.\"\n",
    "sample_embedding = gemini_embeddings.embed_query(sample_text, output_dimensionality=768)\n",
    "\n",
    "print(f\"‚úì Embeddings model initialized: gemini-embedding-001\")\n",
    "print(f\"‚úì Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"‚úì Sample embedding (first 10 values): {sample_embedding[:10]}\")\n",
    "print(f\"\\n‚ÑπÔ∏è  Each chunk will be converted to a {len(sample_embedding)}-dimensional vector for similarity search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "766d5a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index from 49 chunks...\n",
      "This may take a minute depending on the number of chunks...\n",
      "‚úì FAISS vector store created successfully!\n",
      "‚úì Indexed 49 document chunks\n",
      "‚úì Vector store saved to '../vectorstores/faiss_index'\n",
      "\n",
      "‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('../vectorstores/faiss_index', embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS vector store from document chunks\n",
    "# This step converts each chunk to an embedding and stores it\n",
    "print(f\"Creating FAISS index from {len(chunks)} chunks...\")\n",
    "print(\"This may take a minute depending on the number of chunks...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,      # Our split document chunks\n",
    "    embedding=gemini_embeddings   # Gemini embedding model\n",
    ")\n",
    "\n",
    "print(f\"‚úì FAISS vector store created successfully!\")\n",
    "print(f\"‚úì Indexed {len(chunks)} document chunks\")\n",
    "\n",
    "# Save the vector store to disk for later use\n",
    "# This allows you to reload the index without re-processing documents\n",
    "vectorstore_path = \"../vectorstores/faiss_index\"\n",
    "vectorstore.save_local(vectorstore_path)\n",
    "print(f\"‚úì Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\n‚ÑπÔ∏è  You can reload this index later using: FAISS.load_local('{vectorstore_path}', embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea89a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded existing vector store from '../vectorstores/faiss_index'\n"
     ]
    }
   ],
   "source": [
    "# Example: load an existing vector store instead of creating a new one\n",
    "vectorstore_path = \"../vectorstores/faiss_index\"\n",
    "vectorstore = FAISS.load_local(\n",
    "    vectorstore_path, \n",
    "    gemini_embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for loading pickled data\n",
    ")\n",
    "print(f\"‚úì Loaded existing vector store from '{vectorstore_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd75d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Retriever configured successfully\n",
      "  - Search type: similarity\n",
      "  - Number of documents to retrieve (k): 4\n",
      "\n",
      "--- Retriever Test ---\n",
      "Query: 'What is the main topic of this document?'\n",
      "Retrieved 4 documents:\n",
      "\n",
      "Document 1:\n",
      "  Content preview: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "\n",
      "Document 2:\n",
      "  Content preview: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}\n",
      "\n",
      "Document 3:\n",
      "  Content preview: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalis...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Document 4:\n",
      "  Content preview: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly establis...\n",
      "  Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",    # Use cosine similarity for search\n",
    "    search_kwargs={\"k\": 4}        # Retrieve top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úì Retriever configured successfully\")\n",
    "print(f\"  - Search type: similarity\")\n",
    "print(f\"  - Number of documents to retrieve (k): 4\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "# Note: In LangChain 1.0+, use .invoke() instead of .get_relevant_documents()\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(test_query)  # LangChain 1.0+ method\n",
    "\n",
    "print(f\"\\n--- Retriever Test ---\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Content preview: {doc.page_content[:150]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85de8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LLM configured successfully\n",
      "  - Model: gpt-5-nano\n",
      "  - Temperature: 0 (deterministic)\n",
      "  - Max tokens: 2000\n",
      "\n",
      "LLM Test Response: Hello, I am ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "      model=\"gpt-5-nano\",  # Choose your model\n",
    "      # Alternative options:\n",
    "      # model=\"gpt-4o\",           # Faster GPT-4 performance, good \n",
    "      # balance,\n",
    "      # model=\"gpt-3.5-turbo\",    # Faster and cheaper option\n",
    "\n",
    "      temperature=0,         # 0 = deterministic, factual responses (recommended for Q&A)\n",
    "      max_tokens=2000,       # Maximum length of response\n",
    "  )\n",
    "\n",
    "print(\"‚úì LLM configured successfully\")\n",
    "print(f\"  - Model: gpt-5-nano\")\n",
    "print(f\"  - Temperature: 0 (deterministic)\")\n",
    "print(f\"  - Max tokens: 2000\")\n",
    "\n",
    "# Test the LLM with a simple query\n",
    "test_response = llm.invoke(\"Say 'Hello, I am ready to answer questions!'\")\n",
    "print(f\"\\nLLM Test Response: {test_response.content}\")\n",
    "\n",
    "#   üìù Explanation of Parameters:\n",
    "\n",
    "#   Model Selection:\n",
    "\n",
    "#   # Option 1: Best quality (slower, more expensive)\n",
    "#   llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "\n",
    "#   # Option 2: Fast GPT-4 performance (balanced)\n",
    "#   llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "#   # Option 3: Fast and cheap (good for testing)\n",
    "#   llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "#   Temperature:\n",
    "\n",
    "#   temperature=0    # Deterministic, focused (best for factual Q&A)\n",
    "#   temperature=0.7  # More creative, varied responses\n",
    "#   temperature=1.0  # Most creative, less predictable\n",
    "\n",
    "#   Max Tokens:\n",
    "\n",
    "#   max_tokens=2000  # Controls maximum response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8ca95db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the multi head attention?\n",
      "Answer: Multi-Head Attention is a core component of the Transformer architecture, a neural network model that has revolutionized natural language processing and is increasingly used in computer vision and other domains.\n",
      "\n",
      "At its heart, Multi-Head Attention is an extension of the **attention mechanism**, which allows a model to weigh the importance of different parts of an input sequence when processing a specific element.\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "### 1. The Basic Idea: Attention\n",
      "\n",
      "Before Multi-Head, let's understand **Scaled Dot-Product Attention** (often called \"single-head attention\"):\n",
      "\n",
      "1.  **Query (Q), Key (K), Value (V):** For each element in the input sequence, we generate three vectors:\n",
      "    *   **Query (Q):** Represents the current element we are focusing on.\n",
      "    *   **Key (K):** Represents other elements in the sequence that we might want to attend to.\n",
      "    *   **Value (V):** Contains the actual information from other elements that we might want to incorporate.\n",
      "    These Q, K, V vectors are typically derived by applying linear transformations (matrix multiplications) to the input embeddings.\n",
      "\n",
      "2.  **Similarity Scores:** We calculate a similarity score between the Query (Q) and all Keys (K) in the sequence. This is usually done via a dot product: `Q * K^T`. A higher score means the Key is more relevant to the Query.\n",
      "\n",
      "3.  **Scaling:** The scores are divided by the square root of the dimension of the Key vectors (`sqrt(d_k)`) to prevent very large values from pushing the softmax into regions with tiny gradients.\n",
      "\n",
      "4.  **Softmax:** A softmax function is applied to these scaled scores to turn them into probability distributions (attention weights). These weights sum to 1 and indicate how much attention each element in the sequence should receive relative to the current Query.\n",
      "\n",
      "5.  **Weighted Sum:** Finally, these attention weights are multiplied by the Value (V) vectors and summed up. This produces an \"attention output\" or \"context vector\" for the current Query, which is a weighted average of the Value vectors, emphasizing the most relevant ones.\n",
      "\n",
      "This single attention head allows the model to focus on *one type* of relationship or *one aspect* of the input.\n",
      "\n",
      "### 2. The \"Multi-Head\" Extension: Why and How\n",
      "\n",
      "The problem with a single attention head is that it might be limited in its ability to capture diverse relationships within the data. For example, in a sentence, one head might focus on syntactic dependencies (e.g., subject-verb agreement), while another might focus on semantic relationships (e.g., \"apple\" and \"fruit\").\n",
      "\n",
      "**Multi-Head Attention addresses this by:**\n",
      "\n",
      "*   **Running multiple attention mechanisms in parallel.**\n",
      "*   **Each \"head\" learns different linear transformations (Q, K, V projection matrices).** This means each head can learn to look for different kinds of relationships or focus on different parts of the input sequence.\n",
      "\n",
      "**Here's how it works step-by-step:**\n",
      "\n",
      "1.  **Input Transformation:** The input sequence (e.g., word embeddings) is fed into the Multi-Head Attention layer.\n",
      "\n",
      "2.  **Parallel Projections:** Instead of just one set of `W_Q`, `W_K`, `W_V` matrices for the entire layer, Multi-Head Attention has `H` (number of heads) *separate* sets of `W_Qh`, `W_Kh`, `W_Vh` matrices.\n",
      "    *   For each head `h` (from 1 to `H`):\n",
      "        *   The input is linearly projected using `W_Qh`, `W_\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    max_output_tokens=2000\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"What is the multi head attention?\")\n",
    "\n",
    "print((\"Question: What is the multi head attention?\"))\n",
    "print(f\"Answer: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b96c716f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created successfully using LangChain 1.0+ LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question are formatted with prompt template\n",
      "  5. LLM generates answer based on context\n",
      "  6. Answer is parsed and returned to user\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template for the RAG system\n",
    "# This tells the LLM how to use the retrieved context\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LangChain 1.0+ LCEL (LangChain Expression Language)\n",
    "# This uses the pipe operator (|) to chain components together\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve docs and format them\n",
    "        \"question\": RunnablePassthrough()      # Pass through the question\n",
    "    }\n",
    "    | prompt           # Format with prompt template\n",
    "    | llm              # Generate answer with LLM\n",
    "    | StrOutputParser() # Parse output to string\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created successfully using LangChain 1.0+ LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question are formatted with prompt template\")\n",
    "print(\"  5. LLM generates answer based on context\")\n",
    "print(\"  6. Answer is parsed and returned to user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52e6ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the main topic or subject of this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The main topic of this document is the attention mechanism in neural networks, specifically its visualization and application in tasks like anaphora resolution and following long-distance dependencies, often in the context of neural machine translation.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}\n",
      "  Content: (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../resources/pdfs/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}\n",
      "  Content: arXiv:1703.10722, 2017.\n",
      "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXi...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example Query 1: General question about the document\n",
    "query1 = \"What is the main topic or subject of this document?\"\n",
    "\n",
    "print(f\"Query: {query1}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "# With LangChain 1.0+, we invoke the chain with the question directly\n",
    "answer = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# To see which documents were retrieved, we can call the retriever separately\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c85c9eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Can you summarize the key points from this document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "The provided context focuses on attention visualizations within a neural network model. It illustrates how attention mechanisms, specifically encoder self-attention in layer 5, handle linguistic tasks such as following long-distance dependencies (e.g., for the verb 'making' in a phrase like 'making...more difficult') and anaphora resolution (e.g., for the word 'its').\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 2: Specific information extraction\n",
    "query2 = \"Can you summarize the key points from this document?\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(query2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "107e46a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What specific details are mentioned about multi head attention from the document?\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "- MultiHead(Q, K, V) = Concat(head1, ..., headh) WO\n",
      "- headi = Attention(QWQi, KWKi, VWVi)\n",
      "- Projection matrices per head: WQi ‚àà R^{d_model √ó d_k}, WKi ‚àà R^{d_model √ó d_k}, WVi ‚àà R^{d_model √ó d_v}\n",
      "- Output projection: WO ‚àà R^{h d_v √ó d_model}\n",
      "- Number of heads: h = 8\n",
      "- Per-head dimensions: d_k = d_v = d_model / h = 64\n",
      "- Because each head has reduced dimensionality, total computational cost is similar to single-head attention with full dimensionality\n",
      "- The attention used is Scaled Dot-Product Attention, with Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V\n",
      "- In practice, Q, K, V are packed into matrices and processed in parallel across heads.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example Query 3\n",
    "custom_query = \"What specific details are mentioned about multi head attention from the document?\"\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(\"\\nProcessing...\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(custom_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b9f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
